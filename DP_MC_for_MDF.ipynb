{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DP/MC for MDF.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/paxx13/gym/blob/master/DP_MC_for_MDF.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "EUndq7QGj-lk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "this notebook shows dynamic programming techniques to solve a markov devision process(MDP) as well as model free methods."
      ]
    },
    {
      "metadata": {
        "id": "7f_Ff9dcJmTD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pevBr_l8j1Mo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import (absolute_import, division, print_function,\n",
        "unicode_literals)\n",
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from random import randint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6DdGZRGubpSS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "define the environment"
      ]
    },
    {
      "metadata": {
        "id": "oFPNkqolmoty",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v0')\n",
        "env.reset()\n",
        "env.render()\n",
        "'''\n",
        "SFFF       (S: starting point, safe)\n",
        "FHFH       (F: frozen surface, safe)\n",
        "FFFH       (H: hole, fall to your doom)\n",
        "HFFG       (G: goal, where the frisbee is located)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zqcF-xc60vzQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "random search"
      ]
    },
    {
      "metadata": {
        "id": "26oa_3BWnaPc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_random_policy(env):\n",
        "  observation = env.reset()\n",
        "  total_reward = 0\n",
        "  num_steps = 0\n",
        "  for i in range(100):\n",
        "    env.render()\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "\n",
        "    total_reward += reward\n",
        "    num_steps += 1\n",
        "\n",
        "    if done:\n",
        "      print('===========  policy done ==========')\n",
        "      break\n",
        "      \n",
        "  return num_steps, total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "84gtZ0vn_-j1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_params = None\n",
        "best_reward = 0\n",
        "\n",
        "for _ in range(1000):\n",
        "  steps, reward  = run_random_policy(env)\n",
        "  if reward > best_reward:\n",
        "    best_reward = reward\n",
        "    best_steps = steps\n",
        "    if reward == 1:\n",
        "      break\n",
        "      \n",
        "print('best reward: ', best_reward)\n",
        "print('number of steps taken: ', best_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Q0UckDlkK2H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dynamic Programming"
      ]
    },
    {
      "metadata": {
        "id": "VdF_HDmrPdEr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## policy iteration"
      ]
    },
    {
      "metadata": {
        "id": "jeywirzaPaFP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def policy_evaluation(env, policy, gamma, e=1e-10):\n",
        "  v = np.zeros(env.env.nS)\n",
        "  while True:\n",
        "    v_p = np.copy(v)\n",
        "    for s in range(env.env.nS):\n",
        "      a = policy[s]\n",
        "      v[s] = sum([p * (r + gamma*v_p[s_]) for p, s_, r, _ in env.env.P[s][a]])\n",
        "    if (np.sum(np.fabs(v_p-v)) <= e):\n",
        "      break;\n",
        "  return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zOGpLZ3ZSZrO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def policy_improvement(env, v, gamma):\n",
        "  policy = np.zeros(env.env.nS) \n",
        "  for s in range(env.env.nS):\n",
        "    q_sa = np.zeros(env.env.nA)\n",
        "    for a in range(env.env.nA):\n",
        "      for p, s_, r, _ in env.env.P[s][a]:\n",
        "        q_sa[a] = np.sum(p * (r + gamma*v[s_]))\n",
        "    policy[s] = np.argmax(q_sa)\n",
        "  return policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QOZgZ4IXaEzw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, gamma):\n",
        "  policy = np.random.choice(env.env.nA, size=(env.env.nS))\n",
        "  for i in range(1000):\n",
        "    v      = policy_evaluation(env, policy, gamma)\n",
        "    policy_= policy_improvement(env, v, gamma)\n",
        "\n",
        "    if (np.all(policy == policy_)):\n",
        "      break\n",
        "\n",
        "    policy = policy_  \n",
        "  return policy, v "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8DeW-lLjUE3t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gamma = 1.0\n",
        "\n",
        "start_time = time.time()\n",
        "policy, v  = policy_iteration(env, gamma)\n",
        "end_time = time.time()\n",
        "\n",
        "print('optimal value function:\\n', v.reshape(4,4))\n",
        "print('------------------------')\n",
        "print('optimal policy:\\n', policy.reshape(4,4))  \n",
        "print('------------------------')\n",
        "print('time:\\n', end_time - start_time)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RLJpiqaRcn6J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## value iteration"
      ]
    },
    {
      "metadata": {
        "id": "PDAmkShmVIND",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def value_iteration(env, gamma, e=1e-8):\n",
        "  v = np.zeros(env.env.nS)\n",
        "  policy = np.zeros(env.env.nS) \n",
        "  q_sa = np.zeros(env.env.nA)\n",
        "  \n",
        "  for i in range(10000):\n",
        "    v_p = np.copy(v)\n",
        "    for s in range(env.env.nS):\n",
        "      for a in range(env.env.nA):\n",
        "        q_sa[a] = sum([p * (r + gamma*v_p[s_]) for p, s_, r, _ in env.env.P[s][a]])\n",
        "      v[s] = max(q_sa)\n",
        "    if (np.sum(np.fabs(v_p-v)) <= e):\n",
        "      break;\n",
        "  \n",
        "  for s in range(env.env.nS):\n",
        "    for a in range(env.env.nA):\n",
        "        q_sa[a] = sum([p * (r + gamma*v[s_]) for p, s_, r, _ in env.env.P[s][a]])\n",
        "    policy[s] = np.argmax(q_sa)\n",
        "  \n",
        "  return policy, v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "95jXIjHUfKYQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gamma = 1.0\n",
        "\n",
        "start_time = time.time()\n",
        "policy, v  = value_iteration(env, gamma)\n",
        "end_time = time.time()\n",
        "\n",
        "print('optimal value function:\\n', v.reshape(4,4))\n",
        "print('------------------------')\n",
        "print('optimal policy:\\n', policy.reshape(4,4))  \n",
        "print('------------------------')\n",
        "print('time:\\n', end_time - start_time)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s98G8QuJTuZf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Free Control"
      ]
    },
    {
      "metadata": {
        "id": "3hggzlmzaQww",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_progress(e, r,  n=1000):\n",
        "  if e % n == 0:\n",
        "    print('Episode = ', e, ' | Avg Reward = ', r/n )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hrfseLtWdh75",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_policy_from_q(env, q):\n",
        "  p = np.empty(env.env.nS)\n",
        "  for s in range(env.env.nS):\n",
        "    p[s] = (np.argmax(q[s]))\n",
        "  return p    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xc1j-D0ni2oN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Monte Carlo"
      ]
    },
    {
      "metadata": {
        "id": "ctcESWsg24Kz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
        "    \"\"\"\n",
        "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
        "    \n",
        "    Args:\n",
        "        Q: A dictionary that maps from state -> action-values.\n",
        "            Each value is a numpy array of length nA (see below)\n",
        "        epsilon: The probability to select a random action . float between 0 and 1.\n",
        "        nA: Number of actions in the environment.\n",
        "    \n",
        "    Returns:\n",
        "        A function that takes the observation as an argument and returns\n",
        "        the probabilities for each action in the form of a numpy array of length nA.\n",
        "    \n",
        "    \"\"\"\n",
        "    def policy_fn(observation):\n",
        "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
        "        best_action = np.argmax(Q[observation])\n",
        "        A[best_action] += (1.0 - epsilon)\n",
        "        return A\n",
        "    return policy_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JPxr-WqjxE4z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_episode(env, policy, q):\n",
        "    done = False\n",
        "    s = env.reset()\n",
        "    episode= []\n",
        "    while not done:\n",
        "      #probs = policy(s)   \n",
        "      #action = np.random.choice(np.arange(len(probs)), p=probs)\n",
        "      action = epsilon_greedy_policy(env, q, s)\n",
        "      s1 , reward, done, info = env.step(action)\n",
        "      episode.append((s, action, reward))\n",
        "      s = s1\n",
        "        \n",
        "    return episode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-4Ci58Z6gVNt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "  \n",
        "def mc_control(env, gamma, epsilon=0.1, episodes=10000): \n",
        "  q= defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "  policy = make_epsilon_greedy_policy(q, epsilon, env.action_space.n)\n",
        "  returns = defaultdict(float)\n",
        "  returns_count = defaultdict(float)\n",
        "\n",
        "  for e in range(episodes):\n",
        "    G = {} # return on state\n",
        "    C = 0 # cumulative reward\n",
        "    episode = generate_episode(env, policy, q)\n",
        "    avg_reward = 0#np.average(episode[2])\n",
        "    for (state, action, reward) in reversed(episode):\n",
        "      G[(state, action)] = C = reward + gamma*C\n",
        "    for sa in G:\n",
        "      state, action = sa\n",
        "      returns[sa] += G[sa]\n",
        "      returns_count[sa] += 1\n",
        "      q[state][action] = returns[sa] / returns_count[sa]\n",
        "    show_progress(e, avg_reward)\n",
        "  return q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FL1Pqq0v2oh6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "\n",
        "start_time = time.time()\n",
        "q  = mc_control(env, gamma, episodes=100000)\n",
        "end_time = time.time()\n",
        "\n",
        "p = get_policy_from_q(env, q)\n",
        "  \n",
        "print('optimal policy:\\n', p.reshape(4,4))\n",
        "print('------------------------')\n",
        "print('time:\\n', end_time - start_time)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2VkRFh3tT1_C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q Learning"
      ]
    },
    {
      "metadata": {
        "id": "hFXZhvuKbF6-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(env, q, s, epsilon=0.1):\n",
        "  if np.random.uniform(0, 1) < epsilon:\n",
        "    #explore\n",
        "    action = env.action_space.sample() \n",
        "  else:\n",
        "    #exploit\n",
        "    action = np.argmax(q[s]) # Exploit learned values\n",
        "    \n",
        "  return action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Ea6MV9Zap5q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def q_learning(env, alpha, gamma, epsilon=0.1, episodes=10000):\n",
        "  q = np.zeros([env.env.nS, env.env.nA])\n",
        "  \n",
        "  for e in range(episodes):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "      action = epsilon_greedy_policy(env, q, state)\n",
        "      s1, reward, done, _ = env.step(action) \n",
        "      \n",
        "      old_value = q[state, action]\n",
        "      next_max = np.max(q[s1])\n",
        "        \n",
        "      new_value = (1 - alpha) * old_value + alpha * (reward + gamma * s1)\n",
        "      q[state, action] = new_value\n",
        "      \n",
        "      state = s1\n",
        "      \n",
        "    show_progress(e, 0)\n",
        "    \n",
        "  return q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G2QP-FpJ2rkR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "\n",
        "start_time = time.time()\n",
        "q = q_learning(env, alpha, gamma, episodes=500000)\n",
        "end_time = time.time()\n",
        "\n",
        "p = get_policy_from_q(env, q)\n",
        "print('optimal policy:\\n', p.reshape(4,4))\n",
        "print('------------------------')\n",
        "print('time:\\n', end_time - start_time)  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}